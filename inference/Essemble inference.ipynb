{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.2.5)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.3.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.3 MB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.2.5\n",
      "    Uninstalling pandas-1.2.5:\n",
      "      Successfully uninstalled pandas-1.2.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dask-cudf 21.8.3 requires cupy-cuda114, which is not installed.\n",
      "cudf 21.8.3 requires cupy-cuda110, which is not installed.\n",
      "caip-notebooks-serverextension 1.0.0 requires google-cloud-bigquery-storage, which is not installed.\n",
      "pdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.4.3 which is incompatible.\n",
      "dask-cudf 21.8.3 requires dask<=2021.07.1,>=2021.6.0, but you have dask 2021.9.0 which is incompatible.\n",
      "dask-cudf 21.8.3 requires distributed<=2021.07.1,>=2021.6.0, but you have distributed 2021.9.0 which is incompatible.\n",
      "dask-cudf 21.8.3 requires pandas<1.3.0dev0,>=1.0, but you have pandas 1.3.4 which is incompatible.\n",
      "cudf 21.8.3 requires pandas<1.3.0dev0,>=1.0, but you have pandas 1.3.4 which is incompatible.\u001b[0m\n",
      "Successfully installed pandas-1.3.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import  numpy as np\n",
    "import torch\n",
    "import collections\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix,f1_score, accuracy_score,precision_score, recall_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import itertools\n",
    "# testdata=[]\n",
    "\n",
    "# valdata=[]\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('./predictions'):\n",
    "#     for filename in filenames:\n",
    "#         if filename.endswith((\".pkl\",\".tsv\")):\n",
    "#             if \"Valpred\" in filename:\n",
    "#                 valdata.append(os.path.join(dirname, filename))\n",
    "#             elif \"Testpred\" in filename:\n",
    "#                 testdata.append(os.path.join(dirname, filename))\n",
    "#             else:\n",
    "#                 print(os.path.join(dirname, filename))\n",
    "                \n",
    "# testdfs=[torch.load(x) for x in testdata]\n",
    "# valdfs=[torch.load(x) for x in valdata]\n",
    "testpredictions=pd.DataFrame()\n",
    "\n",
    "\n",
    "def getexcludelist(dfs):\n",
    "    \"\"\" exclude not duplicate suid  \"\"\"\n",
    "    valdfsSuid=[df.suid for df in dfs]\n",
    "    excludelist=[]\n",
    "    for i,lst in enumerate(valdfsSuid):\n",
    "        othlst=set()\n",
    "        for j,o in enumerate(valdfsSuid):\n",
    "            if j!=i:\n",
    "                othlst.update(o)\n",
    "        excludelist.extend(list(set(lst).symmetric_difference(set(othlst))))\n",
    "    excludelist=set(excludelist)\n",
    "    print(\"excludelist\" ,len(excludelist))\n",
    "    return excludelist\n",
    "\n",
    "def getConfsid(tdfsinp):\n",
    "    \"\"\"      get ids which has high confidence.    \"\"\"\n",
    "    intersectIdxinfolds=[]\n",
    "    for fold in range(1,6):\n",
    "        for df in tdfsinp:\n",
    "            for fld in range(1,6):\n",
    "                df[f'Tconf{fld}']=df[f'confidence{fld}'].apply(lambda x:np.amax(x)) \n",
    "                df[f'T2confs{fld}']=df[f'confidence{fld}'].apply(lambda x:x[(-x).argsort()[:2]]) \n",
    "                df[f'T2condif{fld}']=df[f'T2confs{fld}'].apply(lambda x:x[0]-x[1])\n",
    "        \n",
    "        tdfs=[df[(df[f'predictions{fold}']!=0) & (df[f'T2condif{fold}']>10)& (df[f'Tconf{fold}']>90)]  for df in tdfsinp]\n",
    "        tdfs=[df.sid.tolist() for df in tdfs]\n",
    "        intersectIdxinfolds.append(set.intersection(*map(set,tdfs)))\n",
    "    print([len(x) for x in intersectIdxinfolds])\n",
    "    combinelist=list(itertools.chain.from_iterable(intersectIdxinfolds))\n",
    "    print(len(combinelist))\n",
    "    return (combinelist)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cleanvaldf(vdfs):\n",
    "    \"\"\"      remove duplicate    \"\"\"\n",
    "    print(\"vdfs len before removing label 0:\",[len(df.index) for df in vdfs])\n",
    "    \n",
    "    vdfs=[df[df.label!=0] for df in vdfs]\n",
    "\n",
    "    print(\"should have same lenght: \",[len(df.index) for df in vdfs])\n",
    "    \n",
    "    excludelist=getexcludelist(vdfs)\n",
    "    print(\"check nonduplicate:\" ,len(excludelist))\n",
    "\n",
    "    print(excludelist)\n",
    "    \n",
    "    vdfs=[df[~df.suid.isin(excludelist)] for df in vdfs]\n",
    "    print(\"removed nonduplicate\")\n",
    "    vdfs=[df.sort_values([\"html_id\", \"sid\"], ascending = (True, True)) for df in vdfs]\n",
    "    \n",
    "    print(\"Same labels:\",[df.label.tolist()==vdfs[0].label.tolist()  for df in vdfs])\n",
    "    \n",
    "    return vdfs\n",
    "\n",
    "def getParams(df):\n",
    "    \"\"\" weight parameter grids \"\"\"\n",
    "    param_grid={}\n",
    "    for i in range(len(valdfs)):\n",
    "        param_grid['wt'+str(i)]=list(range(0,10)) \n",
    "    grid = ParameterGrid(param_grid)\n",
    "    return grid\n",
    "\n",
    "\n",
    "def essembleValidate(dfs,grid):\n",
    "    \"\"\" get the best predictions (for pseudo lable)\"\"\"\n",
    "    label= np.array(dfs[0]['label'].tolist())\n",
    "    preds=[df[[col for col in df if col.startswith('logits')][0]].tolist() for df in dfs]\n",
    "    preds=[pd.DataFrame(np.array(p), columns=['0','1','2','3']) for p in preds]\n",
    "    df = pd.DataFrame([])\n",
    "    preds=np.array([p.to_numpy() for p in preds])\n",
    "    for params in grid:\n",
    "        wts = list(params.values())\n",
    "        wted_preds = np.tensordot(preds, wts, axes=((0),(0)))\n",
    "        wted_ensemble_pred = np.argmax(wted_preds, axis=1)\n",
    "        f1score= f1_score(label, wted_ensemble_pred, average='weighted')   \n",
    "        params['f1score']=f1score\n",
    "        df = df.append(pd.DataFrame(params, index=[0]), ignore_index=True)\n",
    "    \n",
    "    max_acc_row = df.iloc[df['f1score'].idxmax()]\n",
    "    print(\"pred1logitCol:\",[[col for col in df if col.startswith('logits')][0] for df in dfs])\n",
    "    display(max_acc_row)\n",
    "    del max_acc_row[\"f1score\"]\n",
    "    return max_acc_row.tolist()\n",
    "    \n",
    "def essembleTest(tdfs,wts,fold):\n",
    "    \"\"\" get weighted essemble preditions for test\"\"\"\n",
    "\n",
    "    preds=[df[[col for col in df if col.startswith('logits'+str(fold))][0]].tolist() for df in tdfs]\n",
    "    preds=[pd.DataFrame(np.array(p), columns=['0','1','2','3']) for p in preds]\n",
    "    preds=np.array([p.to_numpy() for p in preds])\n",
    "    wted_preds = np.tensordot(preds, wts, axes=((0),(0)))\n",
    "    wted_ensemble_pred = np.argmax(wted_preds, axis=1)\n",
    "#     print(\"predictions summary: \",collections.Counter(wted_ensemble_pred))\n",
    "    return wted_ensemble_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./predictions_PLF/roberta/rFNTestpred.pkl', './predictions_PLF/deberta/DbcawP1/dFNTestpreds.pkl', './predictions_PLF/albert/AbcawP1/aFNTestpreds.pkl', './predictions_PLF/robertapool/rFNTestpreds.pkl']\n"
     ]
    }
   ],
   "source": [
    "testdata=[]\n",
    "valdata=[]\n",
    "for dirname, _, filenames in os.walk('./predictions_PLF'):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith((\".pkl\")):\n",
    "            if \"Valpred\" in filename:\n",
    "                valdata.append(os.path.join(dirname, filename))\n",
    "            elif \"Testpred\" in filename:\n",
    "                testdata.append(os.path.join(dirname, filename))\n",
    "            else:\n",
    "                print(os.path.join(dirname, filename))\n",
    "    \n",
    "# print(\"testdata\")\n",
    "# display(testdata)\n",
    "print(testdata)\n",
    "# testdfs=[torch.load(x) for x in testdata]\n",
    "# print(\"Test size:\",len(testdfs[0].index))\n",
    "# confSid=getConfsid(testdfs)\n",
    "# print(\"High confident size:\",len(confSid))\n",
    "# valdfs=[torch.load(x) for x in valdata]\n",
    "# print(\"valdata\")\n",
    "# display(valdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp=torch.load('./predictions_PLF/roberta/rFNTestpred.pkl')\n",
    "ap=torch.load('./predictions_PLF/albert/AbcawP1/aFNTestpreds.pkl')\n",
    "dp=torch.load('./predictions_PLF/deberta/DbcawP1/dFNTestpreds.pkl')\n",
    "rpp=torch.load('./predictions_PLF/robertapool/rFNTestpreds.pkl')\n",
    "\n",
    "oap=torch.load('./predictions_Otrain/deberta/Dbca/dTestpreds.pkl')\n",
    "odp=torch.load('./predictions_Otrain/albert/Abca/aTestpreds.pkl')\n",
    "\n",
    "rp=rp.sort_values(['sid'], ascending=True)\n",
    "ap=ap.sort_values(['sid'], ascending=True)\n",
    "dp=dp.sort_values(['sid'], ascending=True)\n",
    "rpp=rpp.sort_values(['sid'], ascending=True)\n",
    "oap=oap.sort_values(['sid'], ascending=True)\n",
    "odp=odp.sort_values(['sid'], ascending=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed=np.sum([\n",
    "              np.array(rp.logits1.values.tolist())*0.4,\n",
    "              np.array(rp.logits2.values.tolist())*0.4,\n",
    "              np.array(rp.logits3.values.tolist())*0.4,\n",
    "              np.array(rp.logits4.values.tolist())*0.4,\n",
    "              np.array(rp.logits5.values.tolist())*0.4,\n",
    "\n",
    "              np.array(ap.logits1.values.tolist())*0.1,\n",
    "              np.array(ap.logits2.values.tolist())*0.1,\n",
    "              np.array(ap.logits3.values.tolist())*0.1,\n",
    "              np.array(ap.logits4.values.tolist())*0.1,\n",
    "              np.array(ap.logits5.values.tolist())*0.1,\n",
    "    \n",
    "              np.array(dp.logits1.values.tolist())*0.1,\n",
    "              np.array(dp.logits2.values.tolist())*0.1,\n",
    "              np.array(dp.logits3.values.tolist())*0.1,\n",
    "              np.array(dp.logits4.values.tolist())*0.1,\n",
    "              np.array(dp.logits5.values.tolist())*0.1,\n",
    "    \n",
    "              np.array(rpp.logits1.values.tolist())*0.2,\n",
    "              np.array(rpp.logits2.values.tolist())*0.2,\n",
    "              np.array(rpp.logits3.values.tolist())*0.2,\n",
    "              np.array(rpp.logits4.values.tolist())*0.2,\n",
    "              np.array(rpp.logits5.values.tolist())*0.2,\n",
    "    \n",
    "\n",
    "    ],axis=0)\n",
    "\n",
    "finalpreds=np.argmax(summed,axis=1)\n",
    "\n",
    "cols=[c for c in ap.columns if c.startswith(\"prediction\")]\n",
    "\n",
    "\n",
    "rp=rp[['sid','sentence','predictions1',\"predictions2\",\"predictions3\",\"predictions4\",\"predictions5\"]]\n",
    "rp['final']=finalpreds\n",
    "rp[['al1','al2','al3','al4','al5']]=ap[cols]\n",
    "rp[['dl1','dl2','dl3','dl4','al5']]=dp[cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission=rp[['sid','final']]\n",
    "submission.to_csv('final_submission6.tsv', sep='\\t', header=False, index=False)\n",
    "rp.to_csv('final_submission5.tsv', sep='\\t', header=False, index=False)\n",
    "rp.to_csv('debugfinal_3.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logits1</th>\n",
       "      <th>logits2</th>\n",
       "      <th>logits3</th>\n",
       "      <th>logits4</th>\n",
       "      <th>logits5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6656</th>\n",
       "      <td>[3.0802, -1.9553826, -1.4889772, -0.8823516]</td>\n",
       "      <td>[3.7948003, -1.2423111, -0.84380215, -0.3480488]</td>\n",
       "      <td>[4.380255, -1.8481692, -1.3575437, -0.64343905]</td>\n",
       "      <td>[3.1739135, -1.39625, -1.5155038, -0.055402778]</td>\n",
       "      <td>[3.6735044, -2.1300135, -1.6872667, -0.82474154]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6657</th>\n",
       "      <td>[3.2422812, -2.0478413, -1.6866279, -0.75599223]</td>\n",
       "      <td>[4.5924973, -1.7184323, -1.2202939, -0.52553594]</td>\n",
       "      <td>[4.722586, -2.3896391, -1.8665965, -0.7205733]</td>\n",
       "      <td>[4.1816897, -1.9028848, -1.4969223, -0.13259931]</td>\n",
       "      <td>[4.6876187, -2.5973516, -2.1072855, -0.4646309]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6658</th>\n",
       "      <td>[3.2379122, -2.211926, -1.739107, -0.6793136]</td>\n",
       "      <td>[4.5475307, -1.8399135, -1.1798309, -0.25535828]</td>\n",
       "      <td>[4.711764, -2.5420754, -1.7365987, -0.5063674]</td>\n",
       "      <td>[4.2336035, -1.7477938, -1.4764534, -0.34303597]</td>\n",
       "      <td>[4.402654, -2.565669, -1.8920058, -0.42559606]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6659</th>\n",
       "      <td>[3.2471159, -1.9556475, -1.4725784, -1.0927179]</td>\n",
       "      <td>[4.24002, -1.4342341, -1.2670465, -0.4112149]</td>\n",
       "      <td>[4.300529, -2.3883548, -1.5894133, -0.4265851]</td>\n",
       "      <td>[2.3494687, -1.3481914, -1.7185758, 0.3752105]</td>\n",
       "      <td>[4.3096967, -2.3735752, -1.8924637, -0.8510866]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6660</th>\n",
       "      <td>[2.105722, -1.3781784, -1.2939456, 0.060199153]</td>\n",
       "      <td>[2.087596, -0.7898353, -0.2783787, 0.81896484]</td>\n",
       "      <td>[3.1423414, -1.9518927, -1.0925063, 0.70627433]</td>\n",
       "      <td>[1.8599517, -1.2796087, -1.5876671, 0.5923834]</td>\n",
       "      <td>[1.476131, -1.449832, -1.1187595, 0.32263285]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60502</th>\n",
       "      <td>[3.353171, -2.042561, -1.5702131, -1.0446868]</td>\n",
       "      <td>[4.6403313, -1.7945074, -1.4155688, -0.37161225]</td>\n",
       "      <td>[4.5747943, -2.5091639, -1.8119781, -0.4279667]</td>\n",
       "      <td>[3.8125699, -1.6652552, -1.4479547, -0.25614786]</td>\n",
       "      <td>[4.6241746, -2.353856, -2.0192914, -1.2065104]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60503</th>\n",
       "      <td>[3.3018887, -2.1218963, -1.4532712, -1.0221975]</td>\n",
       "      <td>[4.650549, -1.7644849, -1.3710417, -0.48334587]</td>\n",
       "      <td>[4.18614, -2.5036726, -1.6355969, -0.20441051]</td>\n",
       "      <td>[3.6653295, -1.5191714, -1.2911607, -0.35773224]</td>\n",
       "      <td>[4.793032, -2.3988504, -1.8957782, -1.0807381]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60504</th>\n",
       "      <td>[3.2770944, -1.9039315, -1.444788, -1.0983943]</td>\n",
       "      <td>[4.2270856, -1.4035306, -1.2292924, -0.31876713]</td>\n",
       "      <td>[4.368389, -2.390686, -1.6310568, -0.38876873]</td>\n",
       "      <td>[2.1089015, -1.4324557, -1.7062415, 0.62701416]</td>\n",
       "      <td>[3.798963, -2.2328637, -1.7910721, -0.67908597]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60505</th>\n",
       "      <td>[3.3830113, -1.9724435, -1.565154, -0.900899]</td>\n",
       "      <td>[4.2823515, -1.3597143, -1.3088282, -0.34006852]</td>\n",
       "      <td>[4.4477043, -2.5387573, -1.6693977, -0.54397255]</td>\n",
       "      <td>[3.3986833, -1.9604679, -1.8754586, 0.6565382]</td>\n",
       "      <td>[2.6051064, -1.7207667, -1.723918, 0.075070195]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60506</th>\n",
       "      <td>[3.3107967, -1.9155905, -1.4594465, -0.9501428]</td>\n",
       "      <td>[4.6545324, -1.6996237, -1.472092, -0.4551279]</td>\n",
       "      <td>[4.4909887, -2.3916538, -1.5651737, -0.7711228]</td>\n",
       "      <td>[3.8661885, -1.7180481, -1.2871045, -0.33910924]</td>\n",
       "      <td>[3.4724376, -2.161794, -1.3133926, -0.37235814]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60507 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                logits1  \\\n",
       "6656       [3.0802, -1.9553826, -1.4889772, -0.8823516]   \n",
       "6657   [3.2422812, -2.0478413, -1.6866279, -0.75599223]   \n",
       "6658      [3.2379122, -2.211926, -1.739107, -0.6793136]   \n",
       "6659    [3.2471159, -1.9556475, -1.4725784, -1.0927179]   \n",
       "6660    [2.105722, -1.3781784, -1.2939456, 0.060199153]   \n",
       "...                                                 ...   \n",
       "60502     [3.353171, -2.042561, -1.5702131, -1.0446868]   \n",
       "60503   [3.3018887, -2.1218963, -1.4532712, -1.0221975]   \n",
       "60504    [3.2770944, -1.9039315, -1.444788, -1.0983943]   \n",
       "60505     [3.3830113, -1.9724435, -1.565154, -0.900899]   \n",
       "60506   [3.3107967, -1.9155905, -1.4594465, -0.9501428]   \n",
       "\n",
       "                                                logits2  \\\n",
       "6656   [3.7948003, -1.2423111, -0.84380215, -0.3480488]   \n",
       "6657   [4.5924973, -1.7184323, -1.2202939, -0.52553594]   \n",
       "6658   [4.5475307, -1.8399135, -1.1798309, -0.25535828]   \n",
       "6659      [4.24002, -1.4342341, -1.2670465, -0.4112149]   \n",
       "6660     [2.087596, -0.7898353, -0.2783787, 0.81896484]   \n",
       "...                                                 ...   \n",
       "60502  [4.6403313, -1.7945074, -1.4155688, -0.37161225]   \n",
       "60503   [4.650549, -1.7644849, -1.3710417, -0.48334587]   \n",
       "60504  [4.2270856, -1.4035306, -1.2292924, -0.31876713]   \n",
       "60505  [4.2823515, -1.3597143, -1.3088282, -0.34006852]   \n",
       "60506    [4.6545324, -1.6996237, -1.472092, -0.4551279]   \n",
       "\n",
       "                                                logits3  \\\n",
       "6656    [4.380255, -1.8481692, -1.3575437, -0.64343905]   \n",
       "6657     [4.722586, -2.3896391, -1.8665965, -0.7205733]   \n",
       "6658     [4.711764, -2.5420754, -1.7365987, -0.5063674]   \n",
       "6659     [4.300529, -2.3883548, -1.5894133, -0.4265851]   \n",
       "6660    [3.1423414, -1.9518927, -1.0925063, 0.70627433]   \n",
       "...                                                 ...   \n",
       "60502   [4.5747943, -2.5091639, -1.8119781, -0.4279667]   \n",
       "60503    [4.18614, -2.5036726, -1.6355969, -0.20441051]   \n",
       "60504    [4.368389, -2.390686, -1.6310568, -0.38876873]   \n",
       "60505  [4.4477043, -2.5387573, -1.6693977, -0.54397255]   \n",
       "60506   [4.4909887, -2.3916538, -1.5651737, -0.7711228]   \n",
       "\n",
       "                                                logits4  \\\n",
       "6656    [3.1739135, -1.39625, -1.5155038, -0.055402778]   \n",
       "6657   [4.1816897, -1.9028848, -1.4969223, -0.13259931]   \n",
       "6658   [4.2336035, -1.7477938, -1.4764534, -0.34303597]   \n",
       "6659     [2.3494687, -1.3481914, -1.7185758, 0.3752105]   \n",
       "6660     [1.8599517, -1.2796087, -1.5876671, 0.5923834]   \n",
       "...                                                 ...   \n",
       "60502  [3.8125699, -1.6652552, -1.4479547, -0.25614786]   \n",
       "60503  [3.6653295, -1.5191714, -1.2911607, -0.35773224]   \n",
       "60504   [2.1089015, -1.4324557, -1.7062415, 0.62701416]   \n",
       "60505    [3.3986833, -1.9604679, -1.8754586, 0.6565382]   \n",
       "60506  [3.8661885, -1.7180481, -1.2871045, -0.33910924]   \n",
       "\n",
       "                                                logits5  \n",
       "6656   [3.6735044, -2.1300135, -1.6872667, -0.82474154]  \n",
       "6657    [4.6876187, -2.5973516, -2.1072855, -0.4646309]  \n",
       "6658     [4.402654, -2.565669, -1.8920058, -0.42559606]  \n",
       "6659    [4.3096967, -2.3735752, -1.8924637, -0.8510866]  \n",
       "6660      [1.476131, -1.449832, -1.1187595, 0.32263285]  \n",
       "...                                                 ...  \n",
       "60502    [4.6241746, -2.353856, -2.0192914, -1.2065104]  \n",
       "60503    [4.793032, -2.3988504, -1.8957782, -1.0807381]  \n",
       "60504   [3.798963, -2.2328637, -1.7910721, -0.67908597]  \n",
       "60505   [2.6051064, -1.7207667, -1.723918, 0.075070195]  \n",
       "60506   [3.4724376, -2.161794, -1.3133926, -0.37235814]  \n",
       "\n",
       "[60507 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp['submission']=finalpreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=testdfs[0].columns\n",
    "cols=[c for c in testdfs[0].columns if c.startswith(\"prediction\")]\n",
    "testdfs=[df[cols] for df in testdfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold : 1\n",
      "valdataf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./predictions_PL1/deberta/Dbca_pl1/dValpred1.pkl',\n",
       " './predictions_PL1/deberta/Dbca_pl1 (copy)/dValpred1.pkl']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vdfs len before removing label 0: [1293, 1293]\n",
      "should have same lenght:  [449, 449]\n",
      "excludelist 0\n",
      "check nonduplicate: 0\n",
      "set()\n",
      "removed nonduplicate\n",
      "Same labels: [True, True]\n",
      "should have same lenght:  [449, 449]\n",
      "pred1logitCol: ['logits1_f0.853_v0.033_dbca', 'logits1_f0.853_v0.033_dbca']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wt0        0.000000\n",
       "wt1        1.000000\n",
       "f1score    0.832427\n",
       "Name: 1, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold : 2\n",
      "valdataf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./predictions_PL1/deberta/Dbca_pl1/dValpred2.pkl',\n",
       " './predictions_PL1/deberta/Dbca_pl1 (copy)/dValpred2.pkl']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vdfs len before removing label 0: [1411, 1411]\n",
      "should have same lenght:  [483, 483]\n",
      "excludelist 0\n",
      "check nonduplicate: 0\n",
      "set()\n",
      "removed nonduplicate\n",
      "Same labels: [True, True]\n",
      "should have same lenght:  [483, 483]\n",
      "pred1logitCol: ['logits2_f0.873_v0.028_dbca', 'logits2_f0.873_v0.028_dbca']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wt0        0.000000\n",
       "wt1        1.000000\n",
       "f1score    0.921945\n",
       "Name: 1, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold : 3\n",
      "valdataf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./predictions_PL1/deberta/Dbca_pl1/dValpred3.pkl',\n",
       " './predictions_PL1/deberta/Dbca_pl1 (copy)/dValpred3.pkl']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vdfs len before removing label 0: [1309, 1309]\n",
      "should have same lenght:  [493, 493]\n",
      "excludelist 0\n",
      "check nonduplicate: 0\n",
      "set()\n",
      "removed nonduplicate\n",
      "Same labels: [True, True]\n",
      "should have same lenght:  [493, 493]\n",
      "pred1logitCol: ['logits3_f0.819_v0.046_dbca', 'logits3_f0.819_v0.046_dbca']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wt0        0.000000\n",
       "wt1        1.000000\n",
       "f1score    0.814514\n",
       "Name: 1, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold : 4\n",
      "valdataf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./predictions_PL1/deberta/Dbca_pl1/dValpred4.pkl',\n",
       " './predictions_PL1/deberta/Dbca_pl1 (copy)/dValpred4.pkl']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vdfs len before removing label 0: [372, 372]\n",
      "should have same lenght:  [182, 182]\n",
      "excludelist 0\n",
      "check nonduplicate: 0\n",
      "set()\n",
      "removed nonduplicate\n",
      "Same labels: [True, True]\n",
      "should have same lenght:  [182, 182]\n",
      "pred1logitCol: ['logits4_f0.827_v0.036_dbca', 'logits4_f0.827_v0.036_dbca']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wt0        0.00000\n",
       "wt1        1.00000\n",
       "f1score    0.82195\n",
       "Name: 1, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold : 5\n",
      "valdataf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./predictions_PL1/deberta/Dbca_pl1/dValpred5.pkl',\n",
       " './predictions_PL1/deberta/Dbca_pl1 (copy)/dValpred5.pkl']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vdfs len before removing label 0: [424, 424]\n",
      "should have same lenght:  [218, 218]\n",
      "excludelist 0\n",
      "check nonduplicate: 0\n",
      "set()\n",
      "removed nonduplicate\n",
      "Same labels: [True, True]\n",
      "should have same lenght:  [218, 218]\n",
      "pred1logitCol: ['logits5_f0.83_v0.038_dbca', 'logits5_f0.83_v0.038_dbca']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wt0        0.000000\n",
       "wt1        1.000000\n",
       "f1score    0.817788\n",
       "Name: 1, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pred count\n",
      "0    23742\n",
      "3     3713\n",
      "2      366\n",
      "1      181\n",
      "Name: prediction, dtype: int64\n",
      "High conf count\n",
      "0    23742\n",
      "3     3713\n",
      "2      366\n",
      "1      181\n",
      "Name: prediction, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for fold in range(1,6):\n",
    "for fold in range(1,6):\n",
    "    print(\"fold :\",fold)\n",
    "    valdataf=[val for val in valdata if f\"pred{fold}\" in val]\n",
    "    print(\"valdataf\")\n",
    "    display(valdataf)\n",
    "    valdfs=[torch.load(x) for x in valdataf]\n",
    "    valdfs=cleanvaldf(valdfs)\n",
    "    print(\"should have same lenght: \",[len(df.index) for df in valdfs])\n",
    "    grid=getParams(valdfs)\n",
    "    weights=essembleValidate(valdfs,grid)  \n",
    "    testpredictions[fold]=essembleTest(testdfs,weights,fold)  \n",
    "    \n",
    "\n",
    "testpredictions['prediction']=testpredictions.mode(axis=1).iloc[:, 0]\n",
    "testpredictions['prediction']=testpredictions['prediction'].astype('int32')\n",
    "column=['suid', 'sid', 'html_id', 'ori_sentence', 'title', 'tid', 'random',\n",
    "       'asentence', 'atid', 'arandom', 'bsentence', 'btid', 'brandom',\n",
    "       'sentence']\n",
    "\n",
    "testpredictions[column]=testdfs[0][column]\n",
    "\n",
    "# testp=testpredictions[testpredictions.sid.isin(confSid)]\n",
    "# testp=testp[testp.prediction!=0]\n",
    "testp=testpredictions\n",
    "testp['Hconf']=testpredictions.sid.apply(lambda x: x in confSid)\n",
    "print(\"All pred count\")\n",
    "print(testpredictions.prediction.value_counts())\n",
    "print(\"High conf count\")\n",
    "print(testp.prediction.value_counts())\n",
    "testp=testp.sort_values(['html_id', 'sid'], ascending=[True, True])\n",
    "testp=testp.rename({'prediction': 'label'}, axis=1)\n",
    "testp.drop([1,2,3,4,5], axis=1, inplace=True)\n",
    "testp.to_csv(\"ESBtest1predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns=['suid','logits1','predictions1','confidence1','sentence']\n",
    "# df=testdfs[0][columns]\n",
    "# for fld in range(1,6):\n",
    "#     df[f'Tconf']=df[f'confidence1'].apply(lambda x:np.amax(x)) \n",
    "#     df[f'T2confs1']=df[f'confidence1'].apply(lambda x:x[(-x).argsort()[:2]]) \n",
    "#     df[f'T2condif1']=df[f'T2confs1'].apply(lambda x:x[0]-x[1])\n",
    "# # tdfs=[df[(df[f'predictions1']==1) & (df[f'Tcondif1{fold}']>0)& (df[f'Tconfidence{fold}']>1)]  for df in tdfsinp]\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for df in testdfs:\n",
    "#     for fld in range(1,6):\n",
    "#         df[f'Tpreds{fld}']=df[f'confidence{fld}'].apply(lambda x:(-x).argsort()[:2]) \n",
    "#         df[f'Tconfs{fld}']=df[f'confidence{fld}'].apply(lambda x:x[(-x).argsort()[:2]]) \n",
    "#         df[f'Tcondif{fld}']=df[f'Tconfs{fld}'].apply(lambda x:x[0]-x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # a = np.array([96.4494288 ,  9.99175012, 22.66806066, 46.49918377])\n",
    "# # ind = (-a).argsort()[:2]\n",
    "# # print(ind)\n",
    "# # a[ind]\n",
    "# # x=a\n",
    "testpredictions=rp\n",
    "testpredictions=testpredictions.sort_values(['sid'], ascending=True)\n",
    "submission=testpredictions[['sid','submission']]\n",
    "submission.to_csv('final_submission2.tsv', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rp.to_csv('debugfinal_submission1.tsv', sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
