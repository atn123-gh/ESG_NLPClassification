{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6a76e35",
   "metadata": {},
   "source": [
    "Generate new sentences and remap the newly created sentences to orignial corresponding ID from train and test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba2fc2",
   "metadata": {},
   "source": [
    "#### Compress\n",
    "\n",
    "Create new sentences of length (~450) by joining them to make it longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b60fdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "import re\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import unicodedata\n",
    "\n",
    "#note\n",
    "#check noIds\n",
    "\n",
    "def splitsentence(df,long=True):\n",
    "    if long:\n",
    "        if df['wordcount']>450:\n",
    "            # at least 2 sids after concatenation\n",
    "            if len(str(df['sid']))>15:\n",
    "                num=int(len(df['sentence'])/2)\n",
    "                sentence=[\"\".join(df['sentence'][:num]),\"\".join(df['sentence'][num:])]\n",
    "            else: #\n",
    "                num=int(df['sentence'][0].count(\" \")/2)\n",
    "                txt=df['sentence'][0].split(' ')\n",
    "                sentence=[' '.join(txt[: num]), ' '.join(txt[num: ])]\n",
    "        else:\n",
    "            sentence= [\"\".join(df['sentence'])]\n",
    "    else:\n",
    "            if len(str(df['sid']))>15:\n",
    "                L=2\n",
    "                S =1 \n",
    "                a=np.array(df['sentence'])\n",
    "                nrows = ((a.size-L)//S)+1\n",
    "                sentence=a[S*np.arange(nrows)[:,None] + np.arange(L)]\n",
    "                sentence=[\"\".join(s) for s in sentence]\n",
    "            else: #\n",
    "                sentence =df['sentence']\n",
    "\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a7c5798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_csv(\"../3TrainCompressManualClean.tsv\", sep='\\t')\n",
    "# df['sentence'] = df['sentence'].apply(lambda x: unicodedata.normalize(\"NFKD\",x)) \n",
    "# df['sentence'] = df['sentence'].apply(lambda x: ' '.join(x.split())) # remove multiple space\n",
    "# df['title'] = df['title'].apply(lambda x: unicodedata.normalize(\"NFKD\",x)) \n",
    "# df['title'] = df['title'].apply(lambda x: ' '.join(x.split())) # remove multiple space\n",
    "# df=df.sort_values(['html_id', 'sid'], ascending=[True, True]).reset_index(drop=True)\n",
    "# df.to_csv(\"3TrainCompressManualClean_space.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f314e970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_pickle(\"../zfinal_testdf.pkl\")\n",
    "\n",
    "# noidshtml=df[df['paragraph_id']=='NoID']['html_id'].value_counts().index.tolist()\n",
    "# pl=list(set(df.html_id.value_counts().index.tolist())-set(noidshtml))\n",
    "# print(pl[:10])\n",
    "# print(pl[10:20])\n",
    "# print(pl[20:30])\n",
    "# plSet=pd.DataFrame()\n",
    "# plSet[0]=pl[:10]\n",
    "# plSet[1]=pl[10:20]\n",
    "# plSet[2]=pl[20:30]\n",
    "# plSet[3]=list(noidshtml)+pl[:2]\n",
    "# plSet.to_pickle(\"PLSets.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873dc1e2",
   "metadata": {},
   "source": [
    "#### Concat Paragraph\n",
    "\n",
    "Concat similar sentences according to same paragraph id, same label, same id group etc.</br>\n",
    "Remap the newly created sentences data with id from training and testing file.</br>\n",
    "Create for train data(+label),test data(-label),validation data(-label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae116e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testparagraphconcat\n",
    "def extendParaConcat(df,uselabel=False):\n",
    "    print(\"before  :\" ,df.shape)\n",
    "    if uselabel:\n",
    "        df = df.set_index(['uid','sentence','html_id','title','label'])['sid'].apply(pd.Series).stack().reset_index()\n",
    "    else:\n",
    "        df = df.set_index(['uid','sentence','html_id','title'])['sid'].apply(pd.Series).stack().reset_index()\n",
    "        df=df.rename(columns={0:\"sid\"})\n",
    "        display(df.head(3))\n",
    "        df=df.sort_values(['html_id', 'sid'], ascending=[True, True]).reset_index(drop=True)\n",
    "\n",
    "        \n",
    "    print(\"after  :\" ,df.shape)\n",
    "    return df\n",
    "    \n",
    "def paraConcat(df,uselabel=False):\n",
    "    print(\"before\",df.shape)\n",
    "    \n",
    "    dfID=df[df['paragraph_id']!='NoID']\n",
    "    # df.to_csv(\"1testTP_space.tsv\", sep='\\t')\n",
    "    dfID['sentence'] = dfID['sentence'].apply(lambda x: unicodedata.normalize(\"NFKD\",x)) \n",
    "    dfID['sentence'] =dfID['sentence'].apply(lambda x: str(x).strip())\n",
    "    dfID['uid']=dfID['html_id']+dfID['title'].apply(lambda x:\"\".join([w[0] for w in x.split() if w[0].isalnum() ]))+dfID['paragraph_id']\n",
    "    dfID['wordcount']=dfID['sentence'].apply(lambda x:len(str(x).split()))\n",
    "    if uselabel:\n",
    "        dfID=(dfID.groupby(['uid'])\n",
    "          .agg({'title':'first','html_id':'first','label':pd.Series.mode,'sentence':''.join,'sid':lambda x: list(x.values)})\n",
    "            .reset_index('uid'))\n",
    "        dfID.label=dfID.label.apply(lambda x:x[-1] if type(x) is np.ndarray else x)\n",
    "        print(dfID.label.value_counts())\n",
    "    else:\n",
    "        dfID=(dfID.groupby(['uid'])\n",
    "          .agg({'title':'first','html_id':'first','sentence':''.join,'sid':lambda x: list(x.values)})\n",
    "            .reset_index('uid'))\n",
    "    dfNID=df[df['paragraph_id']=='NoID']\n",
    "    dfNID.sid=dfNID.sid.apply(lambda x:list(str(x).split()))\n",
    "    df=pd.concat([dfID,dfNID])\n",
    "    print(\"after\",df.shape)\n",
    "    print(\"need to do extendParaConcat\")\n",
    "    return df\n",
    "    \n",
    "# traindf=pd.read_pickle(\"../ztraindf.pkl\")\n",
    "# traindf=paraConcat(traindf,uselabel=True) # same for val and test\n",
    "# traindf=extendParaConcat(traindf)\n",
    "# traindf.to_csv(\"5Train_ParaConcat2\"+\".tsv\", sep='\\t')\n",
    "# traindf.to_pickle(\"5Train_ParaConcat2\"+\".pkl\")\n",
    "\n",
    "    \n",
    "Ftestdf=pd.read_pickle(\"../zfinal_testdf.pkl\")\n",
    "Ftestdf=paraConcat(Ftestdf,uselabel=False) # same for val and test\n",
    "# Ftestdf=extendParaConcat(Ftestdf)\n",
    "\n",
    "# Ftestdf.to_csv(\"5FtestParaConcat\"+\".tsv\", sep='\\t')\n",
    "Ftestdf.to_pickle(\"5Ftest_ParaConcat\"+\".pkl\")\n",
    "\n",
    "# testdf=pd.read_pickle(\"../ztestdf.pkl\")\n",
    "# valdf=pd.read_pickle(\"../ztraindf.pkl\")\n",
    "\n",
    "# paraConcat(testdf,\"5test_ParaConcat\")\n",
    "# paraConcat(valdf,\"5val_ParaConcat\")\n",
    "\n",
    "\n",
    "# paraConcat(valdf,\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910542fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_pickle(\"../traindf.pkl\")\n",
    "df['sentence'] = df['sentence'].apply(lambda x: unicodedata.normalize(\"NFKD\",x)) \n",
    "\n",
    "df['sentence'] =df['sentence'].apply(lambda x: str(x).strip())\n",
    "\n",
    "print(\"make uid\")\n",
    "df['uid']=df['html_id']+df['title'].apply(lambda x:\"\".join([w[0] for w in x.split() if w[0].isalnum() ]))+df['paragraph_id']\n",
    "df['wordcount']=df['sentence'].apply(lambda x:len(str(x).split()))\n",
    "df['sentcount_beforesplit']=df['sentence'].apply(lambda x:len(x))\n",
    "print(\"df shape \",df.shape)\n",
    "print(\"exclude title\")\n",
    "notitle=df.loc[(df['isTitle']==0) | (df['wordcount']> 400)].reset_index()\n",
    "\n",
    "print(\"notitle shape \",notitle.shape)\n",
    "display(notitle.head(3))\n",
    "print(\"concat consecutive value group by uid\")\n",
    "blocks = notitle['label'].ne(notitle['label'].shift()).cumsum()\n",
    "notitle=(notitle.groupby(['uid', blocks])\n",
    "  .agg({'label':'first','title':'first','html_id':'first','sentence':lambda x: list(x.values),'sid':','.join})\n",
    "  .reset_index('label', drop=True)\n",
    ")\n",
    "notitle['wordcount']=notitle['sentence'].apply(lambda x:len(str(x).split()))\n",
    "\n",
    "notitle.reset_index(inplace=True)\n",
    "print(\"no title\")\n",
    "display(notitle.head(3))\n",
    "\n",
    "print(\"Shink 0 : otherdf\")\n",
    "otherdf=notitle[notitle.label==0]\n",
    "print(otherdf.shape)\n",
    "print(\"Split sentence\")\n",
    "otherdf['sentcount_beforesplit']=otherdf['sentence'].apply(lambda x:len(x))\n",
    "print(\"otherdf.shape before splitsentence \",otherdf['sentence'].apply(lambda x:len(x)).value_counts())\n",
    "otherdf['sentence']=otherdf.apply(splitsentence,axis=1)\n",
    "print(\"otherdf.shape after splitsentence \",otherdf['sentence'].apply(lambda x:len(x)).value_counts())\n",
    "display(otherdf.head(3))\n",
    "\n",
    "print(\"Shink 123 : esgdf1\")\n",
    "\n",
    "esgdf1=notitle[notitle.label!=0]\n",
    "print(esgdf1.shape)\n",
    "esgdf1['sentcount_beforesplit']=esgdf1['sentence'].apply(lambda x:len(x))\n",
    "print(\"esgdf1.shape before splitsentence \",esgdf1['sentence'].apply(lambda x:len(x)).value_counts())\n",
    "esgdf1['sentence']=esgdf1.apply(splitsentence,axis=1)\n",
    "print(\"esgdf1.shape after splitsentence \",esgdf1['sentence'].apply(lambda x:len(x)).value_counts())\n",
    "display(esgdf1.head(3))\n",
    "\n",
    "print(\"2 pairs 123 : esgdf2\")\n",
    "\n",
    "esgdf2=notitle[notitle.label!=0]\n",
    "print(esgdf2.shape)\n",
    "esgdf2['sentcount_beforesplit']=esgdf2['sentence'].apply(lambda x:len(x))\n",
    "print(\"esgdf2.shape before splitsentence \",esgdf2['sentence'].apply(lambda x:len(x)).value_counts())\n",
    "esgdf2['sentence']=esgdf2.apply(splitsentence,long=False,axis=1)\n",
    "print(\"esgdf2.shape after splitsentence \",esgdf2['sentence'].apply(lambda x:len(x)).value_counts())\n",
    "display(esgdf2.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16609810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extenddf(df):\n",
    "    print(\"before  :\" ,df.shape)\n",
    "    df = df.set_index(['label','uid','sid','html_id','title'])['sentence'].apply(pd.Series).stack()\n",
    "    df = df.reset_index()\n",
    "    df=df.rename(columns={0:\"sentence\"})\n",
    "    display(df.head(3))\n",
    "    return df\n",
    "    \n",
    "    \n",
    "otherdf=extenddf(otherdf)\n",
    "print(\"after  :\" ,otherdf.shape)\n",
    "esgdf1=extenddf(esgdf1)\n",
    "print(\"after  :\" ,esgdf1.shape)\n",
    "esgdf2=extenddf(esgdf2)\n",
    "print(\"after  :\" ,esgdf2.shape)\n",
    "\n",
    "print(\"original shape : \",len(df.index))\n",
    "print(\"original shape 0 :\",len(df[df.label==0].index))\n",
    "print(\"title shape 0 :\",len(df[(df.label==0) & (df.isTitle==1)]))\n",
    "print(\"title shape 123 :\",len(df[(df.label!=0) & (df.isTitle==1)]))\n",
    "\n",
    "print(\"\")\n",
    "print(\"otherdf shape 0:\",len(otherdf.index))\n",
    "\n",
    "print(\"original shape: 123\",df[df.label!=0].shape)\n",
    "print(\"esg1 shape 123:\",len(esgdf1.index))\n",
    "print(\"esg2 shape 123:\",len(esgdf2.index))\n",
    "print(\"esg total:\",len(esgdf1.index)+len(esgdf2.index)+len(df[(df.label!=0) & (df.isTitle==1)]))\n",
    "\n",
    "print(\"Concat data\")\n",
    "columns=[\"uid\",\"label\",\"sentence\",\"title\",\"html_id\",\"sid\"] #\n",
    "traindf=pd.concat([otherdf[columns],esgdf1[columns],esgdf2[columns]])\n",
    "set(traindf['uid']) == set(traindf['uid'].values)\n",
    "traindf=pd.concat([traindf,df[df.isTitle==1][columns]])\n",
    "print(traindf.shape)\n",
    "\n",
    "traindf=traindf.sort_values(['html_id', 'sid'], ascending=[True, True]).reset_index(drop=True)\n",
    "traindf.to_csv(\"compressTrain_title.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687afcd5",
   "metadata": {},
   "source": [
    "#### CONCATAFTER\n",
    " \n",
    "Concatenate with only one next sentences.</br>\n",
    "check if two are ok to joined - same label, have same table, ending periods and start Capitial letter, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe55b0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doConcat(df,uselabel=False):\n",
    "    global duplicatelist\n",
    "    sen2flg=1\n",
    "    sen3flg=1\n",
    "    senlist=[df.sentence]\n",
    "    sidlist=[df.sid]\n",
    "    \n",
    "    if uselabel:\n",
    "        if df.label != df.label2:\n",
    "            return df.sentence,sidlist\n",
    "        if df.label != df.label3:\n",
    "            sen3flg = 0\n",
    "            \n",
    "    if df.title != df.title2:\n",
    "        return df.sentence,sidlist\n",
    "    \n",
    "    if not ((df.start_cap==True ) and (df.hasfullstop==False or df.last_colon==True)):\n",
    "        return df.sentence,sidlist\n",
    "    else:\n",
    "        # if too many cap , assume title and return\n",
    "        words=\" \".join([w for w in df.sentence.split() if not w.lower() in STOPWORDS])\n",
    "        capcount=sum(1 for w in str(words).split() if w[0].isupper())\n",
    "        if capcount>=len(words.split()):\n",
    "            return df.sentence,sidlist\n",
    "\n",
    "    if df.random2:\n",
    "        sen2flg=0\n",
    "    if (df.random3 or df.sentence2[-1]=='.' or df.sentence3[0].isupper()) :\n",
    "        sen3flg=0\n",
    "\n",
    "    if sen2flg==1:\n",
    "        senlist.append(df.sentence2)\n",
    "        sidlist.append(df.sid2)\n",
    "        duplicatelist.append(df.sid2)\n",
    "    if sen3flg==1:\n",
    "        senlist.append(df.sentence3)\n",
    "        sidlist.append(df.sid3)\n",
    "        duplicatelist.append(df.sid3)\n",
    "    sentence = ' '.join(senlist)\n",
    "    \n",
    "    return sentence,sidlist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7403527",
   "metadata": {},
   "source": [
    "#### Concat previous sentence + current sentence + after sentence.\n",
    "\n",
    "check if two are ok to joined - same label, have same table, ending periods and start Capitial letter, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d810ac08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65963, 7)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf=pd.read_pickle(\"../ztraindf.pkl\")\n",
    "traindf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0eb9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def concatCSS(df,outpufilename,uselabel=False):\n",
    "    global duplicatelist\n",
    "    df['sentence'] = df['sentence'].apply(lambda x: unicodedata.normalize(\"NFKD\",x)) \n",
    "    df['sentence'] = df['sentence'].apply(lambda x: ' '.join(x.split())) # remove multiple space\n",
    "#     df['suid']=df['html_id']+df['sid']\n",
    "    \n",
    "    tdf=df.copy()\n",
    "    tdf['hasfullstop'] = tdf['sentence'].apply(lambda x: str(x)[-1]==\".\")\n",
    "    tdf['wordcount']=tdf['sentence'].apply(lambda x:len(str(x).split()))\n",
    "    tdf['stop_word_count'] = tdf['sentence'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "    tdf['cap_word_per'] = tdf['sentence'].apply(lambda x: sum(1 for w in str(x).split() if w[0].isupper())/len(str(x).split()))\n",
    "    tdf['start_num'] = tdf['sentence'].apply(lambda x: str(x)[0].isdigit())\n",
    "    tdf['start_quote']=tdf['sentence'].apply(lambda x: str(x)[0]=='\\\"')\n",
    "    tdf['last_colon'] = tdf['sentence'].apply(lambda x: str(x)[-1]==\":\")    \n",
    "    tdf['start_cap'] = tdf['sentence'].apply(lambda x: str(x)[0].isupper())\n",
    "    tdf['start_isalnum'] = tdf['sentence'].apply(lambda x:str(x)[0].isalnum())\n",
    "    tdf['random'] =tdf['sentence'].apply(lambda x:((x==\"\") or (len(x)-len(re.findall(\"[a-zA-Z$%]\", x)))>=len(re.findall(\"[a-zA-Z$%]\", x))))\n",
    "    \n",
    "    if uselabel:\n",
    "        cols=['sentence','isTitle','title','label','sid','start_cap','random']\n",
    "        tdf[['sentence2','isTitle2','title2','label2','sid2','start_cap2','random2']]=tdf[cols].shift(periods=-1, fill_value=np.nan)\n",
    "        tdf[['sentence3','isTitle3','title3','label3','sid3','start_cap3','random3']]=tdf[cols].shift(periods=-2, fill_value=np.nan)\n",
    "    else:\n",
    "        cols=['sentence','isTitle','title','sid','start_cap','random']\n",
    "        tdf[['sentence2','isTitle2','title2','sid2','start_cap2','random2']]=tdf[cols].shift(periods=-1, fill_value=np.nan)\n",
    "        tdf[['sentence3','isTitle3','title3','sid3','start_cap3','random3']]=tdf[cols].shift(periods=-2, fill_value=np.nan)\n",
    "    \n",
    "    tdf.reset_index(inplace=True,drop=True)\n",
    "    oklist=[]\n",
    "    oklist.extend(tdf[(tdf.start_cap==1)& (tdf.hasfullstop==1)].sentence.index.tolist())\n",
    "    oklist.extend(tdf[tdf.isTitle==1].sentence.index.tolist())\n",
    "    oklist.extend(tdf[tdf.start_num==1].sentence.index.tolist())\n",
    "    oklist.extend(tdf[tdf.random==True].sentence.index.tolist())\n",
    "    oklist=list(set(oklist))\n",
    "    tdf=tdf.drop(tdf.index[oklist])\n",
    "    tdf.reset_index(inplace=True,drop=True)\n",
    "    print(\"afterdroping oklist\",tdf.shape)\n",
    "    \n",
    "    ######\n",
    "    print(\"duplicatelist start\")\n",
    "    print(duplicatelist)\n",
    "\n",
    "    print(\"tdf.shape :\",tdf.shape)\n",
    "    print(uselabel)\n",
    "    tdf[['newsentence','sidlist']]=tdf.apply(doConcat,uselabel=uselabel,axis=1, result_type ='expand')  ####### diff\n",
    "    \n",
    "    tdf.to_csv(\"before stacking.tsv\", sep='\\t', index=False) \n",
    "    pass\n",
    "    if uselabel:\n",
    "        tdf.drop(columns=['sentence', 'sentence2','isTitle2','title2','label2','sid2','start_cap2','random2','sentence3','isTitle3','title3','label3','sid3','start_cap3','random3'],inplace=True)\n",
    "    else:\n",
    "        tdf.drop(columns=['sentence', 'sentence2','isTitle2','title2','sid2','start_cap2','random2','sentence3','isTitle3','title3','sid3','start_cap3','random3'],inplace=True)\n",
    "\n",
    "    tdf=tdf.rename({'newsentence': 'sentence'}, axis=1) \n",
    "    print(\"tdf.shape :\",tdf.shape)\n",
    "    \n",
    "    duplicatelist=list(set(duplicatelist))\n",
    "    print(\"duplicatelist delete\")\n",
    "    print(len(duplicatelist))\n",
    "    display(tdf.head(4))\n",
    "    tdf=tdf.drop(tdf[tdf.sid.isin(duplicatelist)].index.tolist())\n",
    "    tdf.reset_index(inplace=True,drop=True)\n",
    "    remain_idx=tdf.index.tolist()\n",
    "\n",
    "    print(\"after tdf.shape :\",tdf.shape)\n",
    "    print(\"before stacking.csv\")\n",
    "    display(tdf.head(4))\n",
    "    if uselabel:\n",
    "        tdf = tdf.set_index(['sentence', 'html_id','label','title','isTitle','paragraph_id'])['sidlist'].apply(pd.Series).stack()\n",
    "    else:\n",
    "        tdf = tdf.set_index(['sentence', 'html_id','title','isTitle','paragraph_id'])['sidlist'].apply(pd.Series).stack()\n",
    "    tdf=tdf.reset_index()\n",
    "    tdf=tdf.rename(columns={0:\"sid\"})\n",
    "    print(\"after stacking\")\n",
    "    display(tdf)\n",
    "    \n",
    "    print(\"after tdf.shape :\",tdf.shape)\n",
    "    \n",
    "    df=df[~df.sid.isin(tdf.sid.tolist())]\n",
    "    if uselabel:\n",
    "        col=['sid','html_id','sentence','label','title','isTitle','paragraph_id']\n",
    "        df=pd.concat([tdf[col],df[col]], axis=0)\n",
    "    else:\n",
    "        col=['sid','html_id','sentence','title','isTitle','paragraph_id']\n",
    "        df=pd.concat([tdf[col],df[col]], axis=0)\n",
    "    \n",
    "    print('after concat with ok list:',df.shape)\n",
    "    df=df.sort_values(['sid'], ascending=True).reset_index(drop=True)\n",
    "    df.to_csv(outpufilename, sep='\\t', index=False) \n",
    "#     df.to_csv(outpufilename, sep='\\t', index=False)  ####### diff\n",
    "duplicatelist=[]\n",
    "FNtestdf=pd.read_pickle(\"../zfinal_testdf.pkl\")\n",
    "concatCSS(FNtestdf,\"ConcatFNTest.tsv\",uselabel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5354de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicatelist=[]\n",
    "traindf=pd.read_pickle(\"../traindf.pkl\")\n",
    "concatCSS(traindf,\"ConcatTrain.tsv\",uselabel=True)\n",
    "\n",
    "duplicatelist=[]\n",
    "valdf=pd.read_pickle(\"../traindf.pkl\")\n",
    "valdf=valdf.drop(['label'],axis=1)\n",
    "concatCSS(valdf,\"ConcatVal.tsv\",uselabel=False)\n",
    "\n",
    "duplicatelist=[]\n",
    "testdf=pd.read_pickle(\"../testdf.pkl\")\n",
    "concatCSS(testdf,\"ConcatTest.tsv\",uselabel=False)\n",
    "\n",
    "\n",
    "duplicatelist=[]\n",
    "FNtestdf=pd.read_pickle(\"../zfinal_testdf.pkl\")\n",
    "concatCSS(FNtestdf,\"ConcatFNTest.tsv\",uselabel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6918059d",
   "metadata": {},
   "source": [
    "#### Current+After or Current+ Before sentence\n",
    "\n",
    " for training model with 2 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6ceeb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "    \n",
    "\n",
    "def combineAB(df):\n",
    "    if df['bsentence']==df['sentence']:\n",
    "        df['bsentence']=\"\"\n",
    "    if df['asentence']==df['sentence']:\n",
    "        df['asentence']=\"\"\n",
    "    return df['bsentence']+\" \"+df['sentence']+\" \"+df['asentence']\n",
    "\n",
    "def makeBAsentence(df,uselabel=False):\n",
    "    asentence=\"\"\n",
    "    bsentence=\"\"\n",
    "    if df['pid']==df['pid']:\n",
    "        if uselabel:\n",
    "            if df['label'] == df['blabel']:\n",
    "                bsentence=df['bsentence']\n",
    "        else:\n",
    "            bsentence=df['bsentence']\n",
    "    if df['tid']== df['atid']:\n",
    "        if uselabel:\n",
    "            if df['label'] == df['alabel']:\n",
    "                asentence=df['asentence']\n",
    "        else:\n",
    "            asentence=df['asentence']\n",
    "    return asentence,bsentence\n",
    "    \n",
    "def BCA(df,filename,uselabel=False):\n",
    "    df['sentence'] = df['sentence'].apply(lambda x: unicodedata.normalize(\"NFKD\",x)) \n",
    "    df['sentence'] = df['sentence'].apply(lambda x: ' '.join(x.split())) # remove multiple space\n",
    "    print(\"make uid\")\n",
    "    \n",
    "    df['pid']=df['html_id']+df['paragraph_id'] \n",
    "    df['tid']=df['html_id']+df['title']\n",
    "    \n",
    "    tdf=df.copy()\n",
    "    tdf['random'] =tdf['sentence'].apply(lambda x:((x==\"\") or (len(x)-len(re.findall(\"[a-zA-Z$%]\", x)))>=len(re.findall(\"[a-zA-Z$%]\", x))))\n",
    "    \n",
    "    \n",
    "    tdf.reset_index(inplace=True,drop=True)\n",
    "    oklist=[]\n",
    "    oklist.extend(tdf[tdf.isTitle==1].sentence.index.tolist())\n",
    "    oklist.extend(tdf[tdf.random==True].sentence.index.tolist())\n",
    "    oklist=list(set(oklist))\n",
    "    tdf=tdf.drop(tdf.index[oklist])\n",
    "    \n",
    "    if uselabel:\n",
    "        cols=['sentence','isTitle','title','label','pid','tid','random']\n",
    "        tdf[['asentence','aisTitle','atitle','alabel','apid','atid','arandom']]=tdf[cols].shift(periods=-1, fill_value=np.nan)\n",
    "        tdf[['bsentence','bisTitle','btitle','blabel','bpid','btid','brandom']]=tdf[cols].shift(periods=1, fill_value=np.nan)\n",
    "    else:\n",
    "        cols=['sentence','isTitle','title','pid','tid','random']\n",
    "        tdf[['asentence','aisTitle','atitle','apid','atid','arandom']]=tdf[cols].shift(periods=-1, fill_value=np.nan)\n",
    "        tdf[['bsentence','bisTitle','btitle','apid','atid','brandom']]=tdf[cols].shift(periods=1, fill_value=np.nan)\n",
    "\n",
    "        \n",
    "    lbcolumns= [col for col in tdf if 'label' in col]\n",
    "    \n",
    "    tdf[[col for col in tdf if 'sentence' in col]]=tdf[[col for col in tdf if 'sentence' in col]].fillna(\"\")\n",
    "    tdf[[col for col in tdf if 'title' in col]]=tdf[[col for col in tdf if 'title' in col]].fillna(\"\")\n",
    "    tdf[[col for col in tdf if 'pid' in col]]=tdf[[col for col in tdf if 'pid' in col]].fillna(\"\")\n",
    "    tdf[[col for col in tdf if 'tid' in col]]=tdf[[col for col in tdf if 'tid' in col]].fillna(\"\")\n",
    "    tdf[[col for col in tdf if 'random' in col]]=tdf[[col for col in tdf if 'random' in col]].fillna(\"\")\n",
    "    tdf[[col for col in tdf if 'isTitle' in col]]=tdf[[col for col in tdf if 'isTitle' in col]].fillna(0)\n",
    "\n",
    "    if tdf[lbcolumns].isnull().values.any():\n",
    "        tdf[lbcolumns]=tdf[lbcolumns].fillna(0)\n",
    "        for col in lbcolumns:\n",
    "            tdf[col] = tdf[col].astype('int')\n",
    "            \n",
    "    tdf.reset_index(inplace=True,drop=True)\n",
    "    tdf[['aasentence','bbsentence']]=tdf.apply(makeBAsentence,uselabel=uselabel,axis=1, result_type ='expand')  ####### diff\n",
    "    tdf.drop(columns=['asentence', 'bsentence'],inplace=True)\n",
    "    tdf=tdf.rename(columns={\"aasentence\":\"asentence\",\"bbsentence\":\"bsentence\"})\n",
    "    tdf['Lsentence']=tdf.apply(combineAB,axis=1) \n",
    "    tdf['Lsentence'].apply(lambda x: ' '.join(x.split())) \n",
    "    tdf=tdf.rename(columns={\"sentence\":\"sentence_ori\"}).rename(columns={\"Lsentence\":\"sentence\"})\n",
    "\n",
    "    concatdf=pd.concat([df[df.index.isin(df.index[oklist])],tdf])\n",
    "    concatdf=concatdf.sort_index()\n",
    "    concatdf=concatdf.sort_values(['sid'], ascending=True).reset_index(drop=True)\n",
    "    print(concatdf.shape)\n",
    "    concatdf.to_csv(filename, sep='\\t', index=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7403527",
   "metadata": {},
   "source": [
    "#### Concat previous sentence + current sentence + after sentence.\n",
    "\n",
    "check if two are ok to joined - same label, have same table, ending periods and start Capitial letter, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "122253df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make uid\n",
      "(65963, 26)\n",
      "make uid\n",
      "(65963, 21)\n",
      "make uid\n",
      "(28437, 21)\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"ConcatTrain.tsv\", sep='\\t') #62sentenceTest|_4TestCat\n",
    "BCA(df,\"BCACatTrain.tsv\",uselabel=True)\n",
    "df=pd.read_csv(\"ConcatVal.tsv\", sep='\\t') \n",
    "BCA(df,\"BCACatVal.tsv\")\n",
    "df=pd.read_csv(\"ConcatTest.tsv\",sep='\\t') \n",
    "BCA(df,\"BCACatTest.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3000b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make uid\n",
      "(60507, 20)\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"ConcatFNTest.tsv\",sep='\\t') \n",
    "BCA(df,\"BCACatFNTest.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd97b089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
